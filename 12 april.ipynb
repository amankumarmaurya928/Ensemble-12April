{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cdda52-87a6-4e95-ad46-714e2942e30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bagging attempts to reduce the chance of overfitting complex models. It trains a large number of “strong” learners in \\n   parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners \\n   together in order to “smooth out” their predictions.\\n   \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Bagging attempts to reduce the chance of overfitting complex models. It trains a large number of “strong” learners in \n",
    "   parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners \n",
    "   together in order to “smooth out” their predictions.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b54bff7-8010-43e4-bdb8-29566bd1cc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bagging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner.\\n   It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure. \\n   One disadvantage of bagging is that it introduces a loss of interpretability of a model.\\n   the advantages:\\n   1.Ease of implementation: Python libraries such as scikit-learn (also known as sklearn) make it easy to combine the\\n   predictions of base learners or estimators to improve model performance. \\n   2.Reduction of variance: Bagging can reduce the variance within a learning algorithm.\\n   Disadvantages of Bagging\\n   It may result in high bias if it is not modelled properly and thus may result in underfitting.\\n   Since we must use multiple models, it becomes computationally expensive and may not be suitable in various use cases.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''Bagging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner.\n",
    "   It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure. \n",
    "   One disadvantage of bagging is that it introduces a loss of interpretability of a model.\n",
    "   the advantages:\n",
    "   1.Ease of implementation: Python libraries such as scikit-learn (also known as sklearn) make it easy to combine the\n",
    "   predictions of base learners or estimators to improve model performance. \n",
    "   2.Reduction of variance: Bagging can reduce the variance within a learning algorithm.\n",
    "   Disadvantages of Bagging\n",
    "   It may result in high bias if it is not modelled properly and thus may result in underfitting.\n",
    "   Since we must use multiple models, it becomes computationally expensive and may not be suitable in various use cases.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a43e6a-d727-4865-a6f4-ba50e2148e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance. \\n   In Bagging, each model receives an equal weight. In Boosting, models are weighed based on their performance.\\n   Bagging reduces the variance without making the predictions biased. This technique acts as a base to many ensemble \\n   techniques so understanding the intuition behind it is crucial.\\n   If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if \\n   our model has large number of parameters then it's going to have high variance and low bias.\\n   \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance. \n",
    "   In Bagging, each model receives an equal weight. In Boosting, models are weighed based on their performance.\n",
    "   Bagging reduces the variance without making the predictions biased. This technique acts as a base to many ensemble \n",
    "   techniques so understanding the intuition behind it is crucial.\n",
    "   If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if \n",
    "   our model has large number of parameters then it's going to have high variance and low bias.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec828a9c-eab2-4f35-bcc7-98c8d4a4497a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boosting, like bagging, can be used for regression as well as for classification problems. Being mainly focused at reducing\\n   bias, the base models that are often considered for boosting are models with low variance but high bias.\\n   Similar to decision tree and random forest, support vector machine can be used in both classification and regression, SVC \\n   (support vector classifier) is for classification problem.\\n   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''Boosting, like bagging, can be used for regression as well as for classification problems. Being mainly focused at reducing\n",
    "   bias, the base models that are often considered for boosting are models with low variance but high bias.\n",
    "   Similar to decision tree and random forest, support vector machine can be used in both classification and regression, SVC \n",
    "   (support vector classifier) is for classification problem.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3bb643-6154-4f80-81ef-8f3c1f99f286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bias-variance trade-off is a challenge we all face while training machine learning algorithms. Bagging is a powerful \\n   ensemble method which helps to reduce variance, and by extension, prevent overfitting. Ensemble methods improve model \\n   precision by using a group (or \"ensemble\") of models which, when combined, outperform individual models when used\\n   separately.\\n   There are no restrictions/guidelines on the number of models. You can start even from 3 models. You can keep the number of\\n   models as a hyperparameter if the training cost is less.\\n   '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''The bias-variance trade-off is a challenge we all face while training machine learning algorithms. Bagging is a powerful \n",
    "   ensemble method which helps to reduce variance, and by extension, prevent overfitting. Ensemble methods improve model \n",
    "   precision by using a group (or \"ensemble\") of models which, when combined, outperform individual models when used\n",
    "   separately.\n",
    "   There are no restrictions/guidelines on the number of models. You can start even from 3 models. You can keep the number of\n",
    "   models as a hyperparameter if the training cost is less.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7375a82-a315-432a-a553-9a9c46868b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance \\n   within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the\\n   individual data points can be chosen more than once.\\n   '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance \n",
    "   within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the\n",
    "   individual data points can be chosen more than once.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef49af5-78eb-4f82-aba4-4ec95d3b3a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
